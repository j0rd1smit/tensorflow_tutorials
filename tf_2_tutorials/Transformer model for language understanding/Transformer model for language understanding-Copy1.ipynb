{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13\n",
    ")\n",
    "\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7905, 5335, 12, 13, 2799, 7877]\n",
      "The original string: Jordi is awesome.\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Jordi is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7905 ----> J\n",
      "5335 ----> ord\n",
      "12 ----> i \n",
      "13 ----> is \n",
      "2799 ----> awesome\n",
      "7877 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "\n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length, tf.size(y) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0721 16:33:45.341778 10656 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0721 16:33:45.345779 10656 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0721 16:33:45.346779 10656 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0721 16:33:45.349780 10656 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0721 16:33:45.351780  8268 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=311410, shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8214, 1259,    5, ...,    0,    0,    0],\n",
       "        [8214,  299,   13, ...,    0,    0,    0],\n",
       "        [8214,   59,    8, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8214,   95,    3, ...,    0,    0,    0],\n",
       "        [8214, 5157,    1, ...,    0,    0,    0],\n",
       "        [8214, 4479, 7990, ...,    0,    0,    0]], dtype=int64)>,\n",
       " <tf.Tensor: id=311411, shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8087,   18,   12, ...,    0,    0,    0],\n",
       "        [8087,  634,   30, ...,    0,    0,    0],\n",
       "        [8087,   16,   13, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8087,   12,   20, ...,    0,    0,    0],\n",
       "        [8087,   17, 4981, ...,    0,    0,    0],\n",
       "        [8087,   12, 5453, ...,    0,    0,    0]], dtype=int64)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions so that we can add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=311427, shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=311443, shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out2(q, k, v):\n",
    "    #mask = create_look_ahead_mask(tf.shape(k)[1])\n",
    "    mask = None\n",
    "    temp_out = tf.keras.layers.Attention(use_scale=False)([q, v, k])\n",
    "    print ('Output is:')\n",
    "    print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out2(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out2(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out2(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.attentation = tf.keras.layers.Attention()\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "     \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention = self.attentation([q,v,k], mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "    tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500)\n",
    "\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 62)), \n",
    "                                       training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n",
    "               rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, training,look_ahead_mask, padding_mask)\n",
    "\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 26, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000)\n",
    "\n",
    "output = sample_decoder(tf.random.uniform((64, 26)), \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False, look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "      def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "                   target_vocab_size, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "      def call(self, inp, tar, training, enc_padding_mask, \n",
    "               look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 26, 8000])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 62))\n",
    "temp_target = tf.random.uniform((64, 26))\n",
    "\n",
    "fn_out = sample_transformer(temp_input, temp_target, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(400000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1, 1, 40)\n",
      "(64, 1, 40, 40)\n",
      "(64, 1, 1, 40)\n",
      "tf.Tensor(\n",
      "[[0. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]], shape=(40, 40), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "enc_padding_mask, combined_mask, dec_padding_mask =  create_masks(pt_batch, en_batch)\n",
    "print(enc_padding_mask.shape)\n",
    "print(combined_mask.shape)\n",
    "print(dec_padding_mask.shape)\n",
    "\n",
    "print(combined_mask[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    <ipython-input-56-86ce64d9dea1>:20 train_step  *\n        predictions, _ = transformer(inp, tar_inp,\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:667 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    <ipython-input-41-dfd2a15a4a88>:17 call  *\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:667 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    <ipython-input-32-f4e3b341e9fe>:30 call  *\n        x = self.enc_layers[i](x, training, mask)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:667 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    <ipython-input-28-b575bb4961f1>:16 call  *\n        attn_output = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:667 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    <ipython-input-24-b32a8f71065c>:40 call  *\n        scaled_attention = self.attentation([q,v,k], mask)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:667 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\dense_attention.py:117 call\n        self._validate_call_args(inputs=inputs, mask=mask)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\dense_attention.py:159 _validate_call_args\n        if mask:\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:698 __bool__\n        raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\n\n    TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-6d6c832cdd05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# inp -> portuguese, tar -> english\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[0minitializer_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    357\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    358\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 359\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1360\u001b[1;33m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1361\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1646\u001b[0m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1648\u001b[1;33m         \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1649\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1650\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   1539\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1540\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1541\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   1542\u001b[0m         self._function_attributes)\n\u001b[0;32m   1543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    714\u001b[0m                                           converted_func)\n\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    704\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-56-86ce64d9dea1>:20 train_step  *\n        predictions, _ = transformer(inp, tar_inp,\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:667 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    <ipython-input-41-dfd2a15a4a88>:17 call  *\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:667 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    <ipython-input-32-f4e3b341e9fe>:30 call  *\n        x = self.enc_layers[i](x, training, mask)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:667 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    <ipython-input-28-b575bb4961f1>:16 call  *\n        attn_output = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:667 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    <ipython-input-24-b32a8f71065c>:40 call  *\n        scaled_attention = self.attentation([q,v,k], mask)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:667 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\dense_attention.py:117 call\n        self._validate_call_args(inputs=inputs, mask=mask)\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\dense_attention.py:159 _validate_call_args\n        if mask:\n    c:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:698 __bool__\n        raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\n\n    TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        if batch % 50 == 0:\n",
    "              print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "\n",
    "    # inp sentence is portuguese, hence adding the start and end token\n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.equal(predicted_id, tokenizer_en.vocab_size+1):\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "    result = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-9c87fa8c1259>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"este  um problema que temos que resolver.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Real translation: this is a problem we have to solve .\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-1ca3f1e1caff>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(sentence, plot)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     predicted_sentence = tokenizer_en.decode([i for i in result \n\u001b[0;32m      5\u001b[0m                                             if i < tokenizer_en.vocab_size])  \n",
      "\u001b[1;32m<ipython-input-61-0b5b255f5947>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(inp_sentence)\u001b[0m\n\u001b[0;32m     22\u001b[0m                                                      \u001b[0menc_padding_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                                                      \u001b[0mcombined_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                                                      dec_padding_mask)\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# select the last word from the seq_len dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-dfd2a15a4a88>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001b[0m\n\u001b[0;32m     15\u001b[0m                look_ahead_mask, dec_padding_mask):\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0menc_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, inp_seq_len, d_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# dec_output.shape == (batch_size, tar_seq_len, d_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-f4e3b341e9fe>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, training, mask)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m  \u001b[1;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-b575bb4961f1>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, training, mask)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mout1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayernorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-b32a8f71065c>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, v, k, q, mask)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mscaled_attention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mscaled_attention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_attention\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, seq_len_q, num_heads, depth)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    711\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 712\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\dense_attention.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    115\u001b[0m   \u001b[1;31m# TODO(b/125916026): Consider exposing a __call__ method with named args.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_call_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\dense_attention.py\u001b[0m in \u001b[0;36m_validate_call_args\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    157\u001b[0m           \u001b[1;34m'namely [query, value] or [query, value, key]. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m           'Given length: {}'.format(class_name, len(inputs)))\n\u001b[1;32m--> 159\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mc:\\users\\jordi\\documents\\github\\test\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 972\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "translate(\"este  um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"vou ento muito rapidamente partilhar convosco algumas histrias de algumas coisas mgicas que aconteceram.\")\n",
    "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"este  o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\n",
    "print (\"Real translation: this is the first book i've ever done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}